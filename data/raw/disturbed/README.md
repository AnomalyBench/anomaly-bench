Disturbed Traces
==============================

Some application executions were manually disturbed by introducing anomalous events during precise periods. 6 types of events (inside 5 types of traces) were considered throughout the experiments: 

* **Bursty Input (Type 1):** *To mimic input rate spikes, we ran a disruptive event generator (DEG) on the Data Senders to temporarily increase the input rate by a given factor for a duration of 15-30 minutes. We repeated this pattern multiple times during a given trace, creating a total of 29 instances of this anomaly type over 6 different traces.*
* **Bursty Input Until Crash (Type 2):** *This is a longer variation of the Type 1 anomaly, where the DEG period lasts forever, crashing the executors due to lack of memory. When an executor crashes, Spark launches a replacement, but the sustained high rates will keep crashing the executors, until eventually Spark decides to kill the whole application. We injected this anomaly into 7 different traces.*
* **Stalled Input (Type 3):** *This anomaly mimics failures of Spark data sources (e.g., Kafka or HDFS). To create it, we ran a DEG that set the input rates to 0 for about 15 minutes, and then periodically repeated this pattern every few hours, giving us a total of 16 anomaly instances across 4 different traces.*
* **CPU Contention (Type 4):** *The YARN resource manager cannot prevent external programs from using the CPU cores that it has allocated to Spark processes, causing scheduling delays to build up due to CPU contention. We reproduced this anomaly using a DEG that ran Python programs to consume all CPU cores available on a given Spark node. We created 26 such anomaly instances over 6 different traces.*

- **Driver Failure (Type 5)** and **Executor Failure (Type 6)**: *Hardware faults or maintenance operations may cause a node to fail all of a sudden, making all processes (drivers and/or executors) located on that node unreachable. Such processes must be restarted on another node, which causes delays. We created such anomalies by failing driver processes, where the number of processed records drops to 0 until the driver comes back up again in about 20 seconds. We also  created anomalies by failing executor processes, which get restarted 10 seconds after the failure, but its effects on metrics such as processing delay may continue longer. We created 9 driver failures and 10 executor failures over 11 different traces*.